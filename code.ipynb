{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXDHW_mjXtMp"
   },
   "source": [
    "# Mount drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20999,
     "status": "ok",
     "timestamp": 1615282633207,
     "user": {
      "displayName": "Q56094077陳香君",
      "photoUrl": "",
      "userId": "10727804849868825735"
     },
     "user_tz": -480
    },
    "id": "FvCyvU3wXU5i",
    "outputId": "1ebe457a-4d34-474b-ea70-4d9fd6765538"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n",
      " aimas2020\n",
      "'Automatic Generation of Topic Labels.gslides'\n",
      "'Colab Notebooks'\n",
      " cvdl2020\n",
      " iir_book.pdf\n",
      " ir_final\n",
      "'Medical AI'\n",
      "'Paper Slides'\n",
      " Q56094077\n",
      " res18_diabete_noaug.pth\n",
      "'Towards Better Text Understanding and Retrieval through Kernel Entity Saliency Modeling.gslides'\n",
      " tsai.ipynb\n",
      " 獎助學金\n",
      " 申請資料\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "!ls /content/gdrive/My\\ Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7xdJXLgX6xs"
   },
   "outputs": [],
   "source": [
    "# !unzip /content/gdrive/MyDrive/Q56094077/snrs/hw1_0319/hw1_data.zip -d /content/gdrive/MyDrive/Q56094077/snrs/hw1_0319"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNPvyQC_ZsBz"
   },
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5086,
     "status": "ok",
     "timestamp": 1615282641207,
     "user": {
      "displayName": "Q56094077陳香君",
      "photoUrl": "",
      "userId": "10727804849868825735"
     },
     "user_tz": -480
    },
    "id": "R9e-OVDBZbU_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "executionInfo": {
     "elapsed": 776,
     "status": "error",
     "timestamp": 1615283001909,
     "user": {
      "displayName": "Q56094077陳香君",
      "photoUrl": "",
      "userId": "10727804849868825735"
     },
     "user_tz": -480
    },
    "id": "jUN8f3VP4u9D",
    "outputId": "9e4523ef-8226-4980-c086-f15bf7f0a33a"
   },
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import torch_geometric.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Setting:\n",
    "    _root = os.getcwd()\n",
    "\n",
    "    _data = os.path.join(_root, \"hw1_data\")\n",
    "\n",
    "    data_synthetic = os.path.join(_data, \"Synthetic\", \"5000\")\n",
    "    data_youtube = os.path.join(_data, \"youtube\")\n",
    "    \n",
    "    \n",
    "     # Create dir for train/test\n",
    "    date_time = datetime.strftime(datetime.now(), \"%Y-%m-%d %H-%M\")\n",
    "    root = os.path.join(_root,  date_time)\n",
    "    if os.path.exists(root):\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(root)\n",
    "\n",
    "\n",
    "    ## Save plt info\n",
    "    train_info_p = os.path.join(root, \"train.json\")\n",
    "    val_info_p = os.path.join(root, \"valid.json\")\n",
    "    test_info_p = os.path.join(root, \"test.json\")\n",
    "\n",
    "    ## Save plt img\n",
    "    result_plt_p = os.path.join(root, \"train_plt.png\")\n",
    "    test_plt_p = os.path.join(root, \"test_plt.png\")\n",
    "    sum_box_p = os.path.join(root, \"sum_box.png\")\n",
    "        \n",
    "\n",
    "    # Setting of training\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    epochs = 10000\n",
    "    batch = 4\n",
    "\n",
    "    c = 3\n",
    "    depth = 5\n",
    "    p = 128 # embedding dimension of hidden state\n",
    "    q = int(p/2)\n",
    "\n",
    "    epochs = 1000\n",
    "    save_model = os.path.join(root, \"weight.pth\")\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = Setting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOu2CKff-W4B"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZdiko1lmiJy"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gU4XwwAu_WkR"
   },
   "source": [
    "- data.x\t节点特征，维度是[num_nodes, num_node_features]。\n",
    "- data.edge_index\t维度是[2, num_edges]，描述图中节点的关联关系，每一列对应的两个元素，分别是边的起点和重点。数据类型是torch.long。需要注意的是，data.edge_index是定义边的节点的张量（tensor），而不是节点的列表（list）。\n",
    "- data.edge_attr\t边的特征矩阵，维度是[num_edges, num_edge_features]\n",
    "- data.y\t训练目标（维度可以是任意的）。对于节点相关的任务，维度为[num_nodes, *]；对于图相关的任务，维度为[1,*]。\n",
    "- data.position\t节点位置矩阵（Node position matrix），维度为[num_nodes, num_dimensions]。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "um9C8SFO_2Tx"
   },
   "source": [
    "- [Learning to Identify High Betweenness Centrality Nodes from\n",
    "Scratch: A Novel Graph Neural Network Approach](https://arxiv.org/pdf/1905.10418.pdf)\n",
    "- node initial feature = [$(d_v), 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5918,
     "status": "ok",
     "timestamp": 1615282793119,
     "user": {
      "displayName": "Q56094077陳香君",
      "photoUrl": "",
      "userId": "10727804849868825735"
     },
     "user_tz": -480
    },
    "id": "cxWp5RNpm1uL",
    "outputId": "4cf7bf3b-d143-4b96-eebb-2cd9c1b0f408"
   },
   "outputs": [],
   "source": [
    "synthetic = []\n",
    "between = []\n",
    "for f in os.listdir(setting.data_synthetic):\n",
    "    if \"score\" in f:\n",
    "        # ground truth of betweenness centrality\n",
    "        p = os.path.join(setting.data_synthetic, f)\n",
    "        between.append(p)\n",
    "    else:\n",
    "        p = os.path.join(setting.data_synthetic, f)\n",
    "        synthetic.append(p)\n",
    "\n",
    "between.sort()\n",
    "synthetic.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "executionInfo": {
     "elapsed": 893,
     "status": "error",
     "timestamp": 1615282795672,
     "user": {
      "displayName": "Q56094077陳香君",
      "photoUrl": "",
      "userId": "10727804849868825735"
     },
     "user_tz": -480
    },
    "id": "GJrGQmiUxRGX",
    "outputId": "81d8b6cb-5008-47d8-baf4-1156d70e07fc"
   },
   "outputs": [],
   "source": [
    "data_list = []\n",
    "\n",
    "for index, f in enumerate(synthetic):\n",
    "    edge_index = torch_geometric.io.read_txt_array(f, dtype=torch.long)\n",
    "    edge_index = edge_index.t().contiguous()\n",
    "    edge_index = utils.to_undirected(edge_index)\n",
    "\n",
    "    row, col = edge_index  \n",
    "    deg = utils.degree(col) # must use col to get degree, why?\n",
    "    deg = deg.numpy()  \n",
    "\n",
    "    vertice = []\n",
    "    for d in deg:\n",
    "        vertice.append([d, 1, 1])\n",
    "    vertice = np.array(vertice, dtype=np.float)\n",
    "    vertice = torch.from_numpy(vertice)\n",
    "    \n",
    "    ### between centrality\n",
    "    bcs = []\n",
    "    bc = torch_geometric.io.read_txt_array(between[index], dtype=torch.double)\n",
    "    bc = bc.t().contiguous()\n",
    "    row, col = bc\n",
    "    bc = col\n",
    "    bc = bc.numpy()\n",
    "    for b in bc:\n",
    "        bcs.append([b])\n",
    "\n",
    "#     bcs = np.array(bcs)\n",
    "    data = Data(x=vertice, edge_index=edge_index, y=bcs)\n",
    "\n",
    "    data_list.append(data)\n",
    "\n",
    "loader = DataLoader(data_list, batch_size=setting.batch)\n",
    "# print(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jh1T_INgi8Ql"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gcEf-tUaqDVa"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_max_pool\n",
    "from torch_geometric.typing import Adj, OptTensor\n",
    "from torch_geometric.transforms import Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 135
    },
    "executionInfo": {
     "elapsed": 661,
     "status": "error",
     "timestamp": 1615203940879,
     "user": {
      "displayName": "Q56094077陳香君",
      "photoUrl": "",
      "userId": "10727804849868825735"
     },
     "user_tz": -480
    },
    "id": "P5EzKoYr7SGL",
    "outputId": "5504fb67-0455-4af0-a697-930410895a70"
   },
   "outputs": [],
   "source": [
    "class Net(MessagePassing):\n",
    "    def __init__(self, c, p, q, num_layers, device, aggr=\"add\"):\n",
    "        super(Net, self).__init__(aggr=aggr)\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.w_0 = torch.nn.Linear(in_features=c, out_features=p).double()\n",
    "        \n",
    "        self.rnn = torch.nn.GRUCell(p, p).double()\n",
    "  \n",
    "        self.w_4 = torch.nn.Linear(in_features=p, out_features=q).double()\n",
    "        self.w_5 = torch.nn.Linear(in_features=q, out_features=1).double()\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # h_0 = x\n",
    "\n",
    "        # h_1\n",
    "        x = self.w_0(x)\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        \n",
    "        row, col = edge_index\n",
    "        deg = utils.degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg = torch.add(deg, 1)\n",
    "        deg_inv_sqrt = torch.pow(deg, -0.5)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        h_s = [x]\n",
    "        \n",
    "        \n",
    "        for i in range(self.num_layers-1):\n",
    "            # internally calls the message(), aggregate() and update() functions\n",
    "            m = self.propagate(edge_index, x=x, norm=norm)\n",
    "            x = self.rnn(m, x)\n",
    "            x = F.normalize(x, p=2, dim=1) \n",
    "           \n",
    "            h_s.append(x)\n",
    "        \n",
    "        h_s = torch.stack(h_s, dim=-1)\n",
    "\n",
    "        # Use torch.max to replace max_pooling\n",
    "        z, _ = torch.max(h_s, dim=-1)\n",
    "        # z = global_max_pool(h_s, torch.tensor([0], dtype=torch.long).to(self.device))\n",
    "        \n",
    "        \n",
    "        ### Decoder\n",
    "        z = self.w_4(z)\n",
    "        z = F.relu(z)\n",
    "        z = self.w_5(z)\n",
    "        \n",
    "        return z\n",
    "\n",
    "    def message(self, x_j, norm: OptTensor):\n",
    "        return x_j if norm is None else norm.view(-1, 1) * x_j\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dvoodu8ki_Cu"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath, device, **params):\n",
    "\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = Net(c=params[\"c\"], p=params[\"p\"], q=params[\"q\"], num_layers=params[\"depth\"], device=device).to(device)\n",
    "\n",
    "    if os.path.exists(filepath):\n",
    "        checkpoint = torch.load(filepath)\n",
    "        model.load_state_dict(checkpoint['model_stat'])\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=5e-4)\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_stat'])\n",
    "\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=5e-4)\n",
    "\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer = load_checkpoint(\n",
    "                    setting.save_model,\n",
    "                    setting.device,\n",
    "                    c=setting.c, \n",
    "                    p=setting.p, \n",
    "                    q = setting.q, \n",
    "                    depth=setting.depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (w_0): Linear(in_features=3, out_features=128, bias=True)\n",
       "  (rnn): GRUCell(128, 128)\n",
       "  (w_4): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (w_5): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n(model, data, n=5):\n",
    "\n",
    "    hit = 0\n",
    "    \n",
    "    for index, row in enumerate(x_test_index):\n",
    "\n",
    "        \n",
    "        \n",
    "        x_in = x_test_input[index]\n",
    "        _user_test = x_in[\"user_input\"]\n",
    "        _movie_test = x_in[\"item_input\"]\n",
    " \n",
    "        movie_index = np.array([ X[row, 1] ], dtype=int)\n",
    "        movie_index = map_to_index(movie_index, movie_unique)        \n",
    "\n",
    "        result = model.predict(x=x_in, batch_size=BATCH_SIZE)\n",
    "        result = np.reshape(result, ITEMS+LEAVE)\n",
    "    \n",
    "        top_n_index = result.argsort()[-TOP_N:]\n",
    "    \n",
    "        if movie_index in _movie_test[top_n_index]:\n",
    "            hit += 1\n",
    "\n",
    "    print(\"total hit: %d, \\t accuracy: %f \" % (hit, hit/len(x_test_index)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info = {\n",
    "       \"bce\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:02<00:00,  3.78it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, loss = 0.6931509327813485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.38it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, loss = 0.6931488629992566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.62it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2, loss = 0.6931497713462723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.42it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3, loss = 0.6931471846216292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.37it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4, loss = 0.6931464541857284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.41it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5, loss = 0.6931461773972476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.59it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 6, loss = 0.6931468715734843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.33it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 7, loss = 0.6931446732864703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.40it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 8, loss = 0.693144645510898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.39it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 9, loss = 0.693143742951595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.54it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 10, loss = 0.6931440721052742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.30it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 11, loss = 0.6931429348508946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.32it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 12, loss = 0.6931432860439365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.37it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 13, loss = 0.6931460573822468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.58it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 14, loss = 0.6931440404870453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.36it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 15, loss = 0.6931431933572934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.35it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 16, loss = 0.6931446744474531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.41it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 17, loss = 0.6931410503026343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.62it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 18, loss = 0.6931455676352883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.38it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 19, loss = 0.6931438753043707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.38it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 20, loss = 0.6931415333917978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.46it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 21, loss = 0.6931413394924253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.39it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 22, loss = 0.6931430973074895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.39it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 23, loss = 0.69314447400693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.39it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 24, loss = 0.6931405055524923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.59it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 25, loss = 0.6931437878977469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.34it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 26, loss = 0.6931432724150339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.39it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 27, loss = 0.6931439221596263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.60it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 28, loss = 0.6931438007469133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.39it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 29, loss = 0.6931413188185316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.35it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 30, loss = 0.6931426531055153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▎        | 1/8 [00:00<00:01,  4.82it/s]"
     ]
    }
   ],
   "source": [
    "min_bce = 10000\n",
    "\n",
    "for epoch in range(setting.epochs):\n",
    "    \n",
    "    bce_loss = 0.0\n",
    "    graph_cnt = 0\n",
    "    for data in tqdm(loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        data = data.to(setting.device)\n",
    "        bc_pr = model(data)\n",
    "        bc_gt = data.y\n",
    "        bc_gt = np.array(bc_gt)\n",
    "        bc_gt = torch.from_numpy(bc_gt).squeeze()\n",
    "        bc_gt = torch.reshape(bc_gt, (-1, ))\n",
    "        \n",
    "        ### random sample 5|V| nodes\n",
    "        src = (torch.rand(25000) * 4999).long()\n",
    "        det = (torch.rand(25000) * 4999).long()\n",
    "        for b in range(len(data.batch)//5000-1):\n",
    "            src = torch.cat((src, (torch.rand(25000) * 4999).long()+(b+1)*5000))\n",
    "            det = torch.cat((det, (torch.rand(25000) * 4999).long()+(b+1)*5000))\n",
    "                        \n",
    "        y_gt = (bc_gt[det] - bc_gt[src]).squeeze().to(setting.device)\n",
    "        y_pr = (bc_pr[det] - bc_pr[src]).squeeze()\n",
    "   \n",
    "        loss = setting.criterion(y_pr, y_gt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        bce_loss += loss.item()\n",
    "        graph_cnt += 1\n",
    "        \n",
    "#         bce_loss = torch.tensor(0, dtype=torch.float).to(setting.device)\n",
    "        \n",
    "#         for b in range(setting.batch):\n",
    "#             index = picked[b]\n",
    "#             for i in range(len(index)):\n",
    "#                 s1, s2 = index[i]\n",
    "                \n",
    "#                 y_gt = bc_gt[b][s2] - bc_gt[b][s1]\n",
    "#                 y_pr = bc_pr[b][s2] - bc_pr[b][s1]\n",
    "                \n",
    "#                 y_gt = torch.from_numpy(y_gt).to(setting.device)\n",
    "#                 loss = setting.criterion(y_pr, y_gt)\n",
    "#                 bce_loss += loss\n",
    "                \n",
    "#         bce_loss += data.num_graphs * loss.item()\n",
    "#         graph_cnt += data.num_graphs\n",
    "        \n",
    "#         bce_loss.backward()\n",
    "        \n",
    "        \n",
    "    l = bce_loss/graph_cnt\n",
    "    print(\"Epoch = {}, loss = {}\".format(epoch, l))\n",
    "    \n",
    "    train_info[\"bce\"].append(l)\n",
    "    with open(setting.train_info_p, 'w') as f:\n",
    "        json.dump(train_info, f)\n",
    "\n",
    "    \n",
    "    if l < min_bce:\n",
    "        checkpoint = {\n",
    "            'model_stat': model.state_dict(),\n",
    "            'optimizer_stat': optimizer.state_dict(),\n",
    "        }\n",
    "        torch.save(checkpoint, setting.save_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test fit model\n",
    "predict = model(data.to(device))\n",
    "predict.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our experiments, we randomly sample 5|V | source nodes and 5|V |\n",
    "target nodes with replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25000, 2])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "picked = (torch.rand(25000, 2) * 4999).long()\n",
    "for b in range(batch-1):\n",
    "    picked = torch.stack((picked, (torch.rand(25000, 2) * 4999).long()))\n",
    "    \n",
    "picked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in loader:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-192-f652143d5d1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'y'"
     ]
    }
   ],
   "source": [
    "(loader[0].y-loader.y[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "code.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
