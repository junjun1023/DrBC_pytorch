{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "therapeutic-latin",
   "metadata": {},
   "source": [
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import time\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import torch_geometric.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkit as nk\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-entry",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Setting:\n",
    "    _root = os.getcwd()\n",
    "\n",
    "    _data = os.path.join(_root, \"hw1_data\")\n",
    "\n",
    "    data_split = os.path.join(_root, \"split.json\")\n",
    "\n",
    "    # testset datapath\n",
    "    data_synthetic = os.path.join(_data, \"Synthetic\", \"5000\")\n",
    "    \n",
    "    data_youtube = os.path.join(_data, \"Real\", \"youtube\")\n",
    "    data_amazon = os.path.join(_data, \"Real\", \"amazon\")\n",
    "    data_dblp = os.path.join(_data, \"Real\", \"dblp\")\n",
    "    data_comlj = os.path.join(_data, \"Real\", \"com-lj\")\n",
    "    \n",
    "    # trainset datapath\n",
    "    data_train = os.path.join(_data, \"train\")\n",
    "    # validset datapath\n",
    "    data_valid = os.path.join(_data, \"valid\")\n",
    "    \n",
    "    \n",
    "    # Setting of training\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    epochs = 500\n",
    "    nodes_cnt = 200\n",
    "    \n",
    "    params_drbc = dict(\n",
    "        # according to source paper\n",
    "        encoder_params = dict(\n",
    "            c = 3,\n",
    "            p = 128,\n",
    "            num_layers = 5,\n",
    "            device = device\n",
    "        ),\n",
    "        decoder_params = dict(\n",
    "            p = 128,\n",
    "            q = 64\n",
    "        )\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "    def __init__(self, date_time=None):\n",
    "        \n",
    "    \n",
    "        # Create dir for train/test\n",
    "        if date_time is None:\n",
    "            date_time = datetime.strftime(datetime.now(), \"%Y-%m-%d %H-%M\")\n",
    "        self.root = os.path.join(self._root, \"result\", date_time)\n",
    "        if os.path.exists(self.root):\n",
    "            pass\n",
    "        else:\n",
    "            os.makedirs(self.root)\n",
    "\n",
    "\n",
    "        ### Save plt info\n",
    "        self.train_info_p = os.path.join(self.root, \"train.json\")\n",
    "        self.val_info_p = os.path.join(self.root, \"valid.json\")\n",
    "        self.test_info_p = os.path.join(self.root, \"test.json\")\n",
    "\n",
    "        ### Save plt img\n",
    "        self.result_plt_p = os.path.join(self.root, \"train_plt.png\")\n",
    "        self.test_plt_p = os.path.join(self.root, \"test_plt.png\")\n",
    "    \n",
    "\n",
    "        self.weight_drbc = os.path.join(self.root, \"drbc.pth\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-lease",
   "metadata": {},
   "source": [
    "#### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-sport",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time = datetime.strftime(datetime.now(), \"%Y-%m-%d %H-%M\")\n",
    "# date_time = \"2021-03-23 00-55\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = Setting(d_name=date_time)\n",
    "\n",
    "setting.root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-middle",
   "metadata": {},
   "source": [
    "# Data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-fraud",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, mode=\"between\"):\n",
    "\n",
    "    assert mode==\"between\" or mode==\"closeness\", \"Unknown centrality mode.\"\n",
    "    \n",
    "    edge_index = []\n",
    "    centrality = []\n",
    "    for f in os.listdir(os.path.join(path, \"graph\")):\n",
    "\n",
    "        p = os.path.join(path, f)\n",
    "        edge_index.append(p)\n",
    "\n",
    "        p = os.path.join(path, mode, f)\n",
    "        centrality.append(p)\n",
    "    \n",
    "    return edge_index, centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-study",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def split_data(path, x, y, replace=False):\n",
    "    if os.path.exists(path) and replace:\n",
    "        pass\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.15)\n",
    "\n",
    "        split = {\n",
    "            \"X_train\": X_train,\n",
    "            \"X_valid\": X_valid,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_train\": y_train,\n",
    "            \"y_valid\": y_valid,\n",
    "            \"y_test\": y_test\n",
    "        }\n",
    "\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(split, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-census",
   "metadata": {},
   "source": [
    "### Cvt 2 Pyg.Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_data(x, y=None, x_sep=None, y_sep=None, usecols=None):\n",
    "    \n",
    "    if x_sep is not None:\n",
    "        edge_index = torch_geometric.io.read_txt_array(x, dtype=torch.long, sep=x_sep)\n",
    "    else:\n",
    "        edge_index = torch_geometric.io.read_txt_array(x, dtype=torch.long)\n",
    "    edge_index = edge_index.t().contiguous()\n",
    "    edge_index = utils.to_undirected(edge_index)\n",
    "\n",
    "    row, col = edge_index  \n",
    "    deg = utils.degree(col) # must use col to get degree, why?\n",
    "    deg = deg.numpy()  \n",
    "\n",
    "    vertice = []\n",
    "    for d in deg:\n",
    "        vertice.append([d, 1, 1])\n",
    "    vertice = np.array(vertice, dtype=np.float)\n",
    "    vertice = torch.from_numpy(vertice)\n",
    "\n",
    "    if y is not None:\n",
    "        ### between centrality\n",
    "        score = np.loadtxt(y, delimiter=y_sep, usecols=usecols)\n",
    "        score = np.reshape(score, (-1, 1))\n",
    "        score = torch.from_numpy(score)\n",
    "\n",
    "        data = Data(x=vertice, edge_index=edge_index, y=score)\n",
    "        \n",
    "    else:\n",
    "        data = Data(x=vertice, edge_index=edge_index)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-baseline",
   "metadata": {},
   "source": [
    "### Cvt 2 Pyg.Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataloader(x, y, batch, y_sep=None, usecols=None):\n",
    "    \n",
    "    data_list = []\n",
    "    for x_, y_ in zip(x, y):\n",
    "        data = to_data(x_, y_, y_sep=y_sep, usecols=usecols)\n",
    "        data_list.append(data)\n",
    "\n",
    "    loader = DataLoader(data_list, batch_size=batch)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-devil",
   "metadata": {},
   "source": [
    "### From networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-contemporary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_networkx(G, score_list=None):\n",
    "    r\"\"\"Converts a :obj:`networkx.Graph` or :obj:`networkx.DiGraph` to a\n",
    "    :class:`torch_geometric.data.Data` instance.\n",
    "\n",
    "    Args:\n",
    "        G (networkx.Graph or networkx.DiGraph): A networkx graph.\n",
    "    \"\"\"\n",
    "\n",
    "    G = nx.convert_node_labels_to_integers(G)\n",
    "    G = G.to_directed() if not nx.is_directed(G) else G\n",
    "    edge_index = torch.LongTensor(list(G.edges)).t().contiguous()\n",
    "   \n",
    "    data = {}\n",
    "\n",
    "    for i, (_, feat_dict) in enumerate(G.nodes(data=True)):\n",
    "        for key, value in feat_dict.items():\n",
    "           \n",
    "            data[str(key)] = [value] if i == 0 else data[str(key)] + [value]\n",
    "\n",
    "    for i, (_, _, feat_dict) in enumerate(G.edges(data=True)):\n",
    "        for key, value in feat_dict.items():\n",
    "            data[str(key)] = [value] if i == 0 else data[str(key)] + [value]\n",
    "\n",
    "    for key, item in data.items():\n",
    "        try:\n",
    "            data[key] = torch.tensor(item)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    data['edge_index'] = edge_index.view(2, -1)\n",
    "    data['x'] = torch.from_numpy(\n",
    "        np.array( [ [G.degree[i], 1, 1] for i in G.nodes()], dtype=np.float ) )\n",
    "    \n",
    "    if score_list is not None:\n",
    "        data['y'] = torch.from_numpy(\n",
    "            np.array( [ [b] for b in score_list ] , dtype=np.float) )\n",
    "    data = torch_geometric.data.Data.from_dict(data)\n",
    "    data.num_nodes = G.number_of_nodes()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-hopkins",
   "metadata": {},
   "source": [
    "### Random generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-appendix",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nx_graph(nodes_cnt):\n",
    "    # Draw network G from distribution D (like the power-law model)\n",
    "    G = nx.generators.random_graphs.powerlaw_cluster_graph(n=nodes_cnt, m=4, p=0.05)\n",
    "    # Calculate each node’s exact BC value bv, ∀v ∈ V\n",
    "    betweenness = nx.algorithms.centrality.betweenness_centrality(G)\n",
    "    \n",
    "    # Convert betweenness dict to list\n",
    "    between = [v for k, v in sorted(betweenness.items(), key=lambda  item: int(item[0]), reverse=False)]\n",
    "    bc = np.array(between)\n",
    "    \n",
    "    closeness = nx.algorithms.centrality.closeness_centrality(G)\n",
    "    closeness = [v for k, v in sorted(closeness.items(), key=lambda item: int(item[0]), reverse=False)]\n",
    "    cc = np.array(closeness)\n",
    "    \n",
    "    return G, bc, cc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-flashing",
   "metadata": {},
   "source": [
    "#### Usage\n",
    "\n",
    "Ex. generate 10,000 to train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_dir = os.path.join(Setting._data, \"train\", str(nodes_cnt))\n",
    "\n",
    "if os.path.exists(to_dir):\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(to_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-administration",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_dir = os.path.join(to_dir, \"graph\")\n",
    "bc_dir = os.path.join(to_dir, \"between\")\n",
    "cc_dir = os.path.join(to_dir, \"closeness\")\n",
    "\n",
    "if os.path.exists(g_dir):\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(g_dir)\n",
    "    \n",
    "if os.path.exists(bc_dir):\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(bc_dir)\n",
    "    \n",
    "if os.path.exists(cc_dir):\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(cc_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-plasma",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    G, bc, cc = generate_nx_graph(Setting.nodes_cnt)\n",
    "    save_name = \"{}.txt\".format(i)\n",
    "    \n",
    "    edge_f = open(os.path.join(g_dir, save_name), \"wb\")\n",
    "    nx.readwrite.edgelist.write_edgelist(G, edge_f, data=False)\n",
    "  \n",
    "    np.savetxt(os.path.join(bc_dir, save_name), bc, fmt=\"%.20f\")\n",
    " \n",
    "    np.savetxt(os.path.join(cc_dir, save_name), cc, fmt=\"%.20f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-butler",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-singing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load data\n",
    "train, train_score = load_data(os.path.join(Setting.data_train, str(Setting.nodes_cnt)), mode=\"closeness\")\n",
    "\n",
    "# 2. Cvt 2 dataloader\n",
    "train_loader = to_dataloader(train, train_score, batch=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-fortune",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-oracle",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-shepherd",
   "metadata": {},
   "source": [
    "### Top N %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_percentage(bc_gt, bc_pr, k):\n",
    "    \n",
    "    if not isinstance(bc_gt, torch.Tensor):\n",
    "        bc_gt = torch.from_numpy(bc_gt)\n",
    "    bc_gt = bc_gt.to(setting.device)\n",
    "    bc_gt = torch.reshape(bc_gt, (-1, ))\n",
    "    \n",
    "    if not isinstance(bc_pr, torch.Tensor):\n",
    "        bc_pr = torch.from_numpy(bc_pr)\n",
    "    bc_pr = bc_pr.to(setting.device)\n",
    "    bc_pr = torch.reshape(bc_pr, (-1, ))\n",
    "    \n",
    "    nodes = bc_gt.size()[0]\n",
    "    k = int(nodes * k / 100)\n",
    "    \n",
    "    gt_value, gt_indice = torch.topk(bc_gt, k)\n",
    "    pr_value, pr_indice = torch.topk(bc_pr, k)\n",
    "\n",
    "    gt_indice = set(gt_indice.cpu().numpy())\n",
    "    pr_indice = set(pr_indice.cpu().numpy())\n",
    "\n",
    "    intersect = len(gt_indice & pr_indice)\n",
    "    top = intersect/k\n",
    "    \n",
    "    return top"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-continuity",
   "metadata": {},
   "source": [
    "### Kendal Tau Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def kendal_tau_distance(bc_gt, bc_pr):\n",
    "    \n",
    "    if isinstance(bc_gt, torch.Tensor):\n",
    "        bc_gt = torch.reshape(bc_gt, (-1, ))\n",
    "        bc_gt = bc_gt.cpu().detach().numpy()\n",
    "        \n",
    "    if isinstance(bc_pr, torch.Tensor):\n",
    "        bc_pr = torch.reshape(bc_pr, (-1, ))\n",
    "        bc_pr = bc_pr.cpu().detach().numpy()\n",
    "    \n",
    "    tau, p_value = stats.kendalltau(bc_gt, bc_pr)\n",
    "    return tau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-ambassador",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-alexandria",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath, device, **params):\n",
    "\n",
    "    model = DrBC(**params[\"drbc\"])\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    if os.path.exists(filepath):\n",
    "        print(\"pretrained finded\")\n",
    "        checkpoint = torch.load(filepath)\n",
    "        model.load_state_dict(checkpoint['model_stat'])\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_stat'])\n",
    "\n",
    "    else:\n",
    "        print(\"use a new optimizer\")\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-ceremony",
   "metadata": {},
   "source": [
    "#### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer = load_checkpoint(\n",
    "                    setting.weight_drbc,\n",
    "                    Setting.device,\n",
    "                    drbc = Setting.params_drbc)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-administrator",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-faith",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader, device):\n",
    "    \n",
    "    model = model.eval().to(device)\n",
    "    \n",
    "    top1_list = []\n",
    "    top5_list = []\n",
    "    top10_list = []\n",
    "    kendal_list = []\n",
    "    loss_list = []\n",
    "    time_list = []\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        start = time.time()\n",
    "        bc_pr = model(batch)\n",
    "        end = time.time()\n",
    "\n",
    "        b_index = batch.batch.cpu().numpy()\n",
    "        b = np.max(b_index) + 1\n",
    "\n",
    "        for b_ in range(b):\n",
    "\n",
    "            indice, = np.where(b_index == b_)\n",
    "\n",
    "\n",
    "            gt = batch.y[indice].squeeze()\n",
    "            pr = bc_pr[indice].squeeze()\n",
    "\n",
    "            # evaluation\n",
    "            top1 = top_n_percentage(gt, pr, k=1)\n",
    "            top5 = top_n_percentage(gt, pr, k=5)\n",
    "            top10 = top_n_percentage(gt, pr, k=10)\n",
    "            kendal = kendal_tau_distance(gt, pr)\n",
    "\n",
    "            # compute loss\n",
    "            src = np.random.choice(len(indice), 5*len(indice), replace=True)\n",
    "            det = np.random.choice(len(indice), 5*len(indice), replace=True)\n",
    "            src = torch.from_numpy(src)\n",
    "            det = torch.from_numpy(det)\n",
    "\n",
    "            y_gt = gt[det] - gt[src]\n",
    "            y_pr = pr[det] - pr[src]\n",
    "\n",
    "            y_gt = nn.Sigmoid()(y_gt)\n",
    "            y_pr = nn.Sigmoid()(y_pr)\n",
    "\n",
    "            loss = nn.BCELoss()(y_pr, y_gt)\n",
    "\n",
    "            top1_list.append(top1)\n",
    "            top5_list.append(top5)\n",
    "            top10_list.append(top10)\n",
    "            kendal_list.append(kendal)\n",
    "            loss_list.append(loss.item())\n",
    "            time_list.append(end-start)\n",
    "    \n",
    "    \n",
    "    return top1_list, top5_list, top10_list, kendal_list, time_list, loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-cleaning",
   "metadata": {},
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-russia",
   "metadata": {},
   "source": [
    "## RK(DIAM)\n",
    "\n",
    "ApproxBetweenness\n",
    "\n",
    "Fast approximation of betweenness centrality through sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-median",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rk(save_path, edge_lists, scores):\n",
    "\n",
    "    with open(save_path, \"w\") as f:\n",
    "        jsn = {\n",
    "            \"top1\": [],\n",
    "            \"top5\": [],\n",
    "            \"top10\": [],\n",
    "            \"kendal\": [],\n",
    "            \"time\": []\n",
    "        }\n",
    "        for edge_list, score in zip(edge_lists, scores):\n",
    "            G_nx = nx.readwrite.edgelist.read_edgelist(edge_list, delimiter=\"\\t\")\n",
    "            G_nk = nk.nxadapter.nx2nk(G_nx)\n",
    "\n",
    "            method = nk.centrality.ApproxBetweenness(G_nk, epsilon=0.1)\n",
    "            start = time.time()\n",
    "            method.run()\n",
    "            end = time.time()\n",
    "            \n",
    "            gt = np.loadtxt(score, usecols=1)\n",
    "                \n",
    "            jsn[\"time\"].append(end-start)\n",
    "            jsn[\"kendal\"].append(kendal_tau_distance(np.array(method.scores()), gt))\n",
    "            jsn[\"top1\"].append(top_n_percentage(np.array(method.scores()), gt, k=1))\n",
    "            jsn[\"top5\"].append(top_n_percentage(np.array(method.scores()), gt, k=5))\n",
    "            jsn[\"top10\"].append(top_n_percentage(np.array(method.scores()), gt, k=10))\n",
    "\n",
    "        json.dump(jsn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-orleans",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_rk(os.path.join(setting.root, \"rk.json\"), edge_lists=xxx, scores=xxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-impossible",
   "metadata": {},
   "source": [
    "## KADABRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-parts",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kadabra(save_path, edge_lists, scores):\n",
    "\n",
    "    with open(save_path, \"w\") as f:\n",
    "        jsn = {\n",
    "            \"top1\": [],\n",
    "            \"top5\": [],\n",
    "            \"top10\": [],\n",
    "            \"kendal\": [],\n",
    "            \"time\": []\n",
    "        }\n",
    "        for edge_list, score in zip(edge_lists, scores):\n",
    "            G_nx = nx.readwrite.edgelist.read_edgelist(edge_list, delimiter=\"\\t\")\n",
    "            G_nk = nk.nxadapter.nx2nk(G_nx)\n",
    "            \n",
    "            method = nk.centrality.KadabraBetweenness(G_nk, 0.05, 0.8)\n",
    "            start = time.time()\n",
    "            method.run()\n",
    "            end = time.time()\n",
    "\n",
    "            gt = np.loadtxt(score, usecols=1)\n",
    "            \n",
    "            jsn[\"time\"].append(end-start)\n",
    "            jsn[\"kendal\"].append(kendal_tau_distance(np.array(method.scores()), gt))\n",
    "            jsn[\"top1\"].append(top_n_percentage(np.array(method.scores()), gt, k=1))\n",
    "            jsn[\"top5\"].append(top_n_percentage(np.array(method.scores()), gt, k=5))\n",
    "            jsn[\"top10\"].append(top_n_percentage(np.array(method.scores()), gt, k=10))\n",
    "\n",
    "        json.dump(jsn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-consultation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_rk(os.path.join(setting.root, \"kadabra.json\"), edge_lists=xxx, scores=xxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-heritage",
   "metadata": {},
   "source": [
    "## KBC\n",
    "\n",
    "clone https://github.com/ecrc/BeBeCA and run kbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-partnership",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kbc(save_path, edge_lists, scores):\n",
    "    \n",
    "    import subprocess\n",
    "    with open(save_path, \"w\") as f:\n",
    "        jsn = {\n",
    "            \"top1\": [],\n",
    "            \"top5\": [],\n",
    "            \"top10\": [],\n",
    "            \"kendal\": [],\n",
    "            \"time\": []\n",
    "        }\n",
    "        for edge_list, score in zip(edge_lists, scores):\n",
    "\n",
    "            base = os.path.splitext(os.path.basename(edge_list))[0]\n",
    "\n",
    "            G_nx = nx.readwrite.edgelist.read_edgelist(edge_list, delimiter=\"\\t\")\n",
    "            \n",
    "            # Cvt 2 pyg.Data to get nodes_cnt and edges_cnt\n",
    "            G_pyg = from_networkx(G_nx)\n",
    "\n",
    "            arr = np.array([G_pyg.x.shape[0], G_pyg.edge_index.shape[1]]).reshape((1, 2))\n",
    "            arr = np.concatenate([arr, G_pyg.edge_index.t().numpy()])\n",
    "\n",
    "            save = os.path.join(\"BeBeCA/Source_Code/5000\", \"{}.txt\".format(base))\n",
    "            save_pr = os.path.join(\"BeBeCA/Source_Code/5000\", \"{}_pr.txt\".format(base))\n",
    "\n",
    "            np.savetxt(save, arr, fmt=\"%d\")\n",
    "\n",
    "            start = time.time()\n",
    "            subprocess.run([\"./BeBeCA/Source_Code/KPATH\", \"2\", save, save_pr])\n",
    "            end = time.time()\n",
    "\n",
    "            pr = np.loadtxt(save_pr, delimiter=\":\", usecols=1)\n",
    "            gt = np.loadtxt(score, usecols=1)\n",
    "\n",
    "            jsn[\"time\"].append(end-start)\n",
    "            jsn[\"kendal\"].append(kendal_tau_distance(gt, pr))\n",
    "            jsn[\"top1\"].append(top_n_percentage(gt, pr, k=1))\n",
    "            jsn[\"top5\"].append(top_n_percentage(gt, pr, k=5))\n",
    "            jsn[\"top10\"].append(top_n_percentage(gt, pr, k=10))\n",
    "\n",
    "        json.dump(jsn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-generic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_rk(os.path.join(setting.root, \"kbc.json\"), edge_lists=xxx, scores=xxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-minutes",
   "metadata": {},
   "source": [
    "## DrBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-bottle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_drbc(model, save_path, edge_lists, scores):\n",
    "    with open(os.path.join(setting.root, \"drbc.json\"), \"w\") as f:\n",
    "\n",
    "        data_loader = to_dataloader(edge_lists, scores)\n",
    "\n",
    "        top1_list, top5_list, top10_list, kendal_list, time_list, loss_list = eval_model(model, data_loader, \"cpu\")\n",
    "        \n",
    "        jsn = {\n",
    "            \"top1\": top1_list,\n",
    "            \"top5\": top5_list,\n",
    "            \"top10\": top10_list,\n",
    "            \"kendal\": kendal_list,\n",
    "            \"time\": time_list\n",
    "        }\n",
    "\n",
    "        json.dump(jsn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-syndication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_rk(os.path.join(setting.root, \"drbc.json\"), edge_lists=xxx, scores=xxx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
